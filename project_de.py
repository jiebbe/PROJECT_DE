# -*- coding: utf-8 -*-
"""Project_DE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11utXaHI-eKDA-GGyuW6rZA8UUz5_CopM

Import all the libraries we need for this project
"""



import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
plt.style.use('ggplot')
pd.set_option('display.max_columns', 200)

"""# Preprocessing

Display the first 5 rows of the dataset with data from years 2001 to 2004
"""

crimes1 = pd.read_csv("/content/sample_data/Chicago_Crimes_2001_to_2004.csv", delimiter=',', engine='python', on_bad_lines='skip')

nRow, nCol = crimes1.shape
print(f'There are {nRow} rows and {nCol} columns')

#The unnamed column in your DataFrame is a result of the CSV file having an index column that Pandas is reading in as an actual column of data.

#drop the index column
crimes1 = crimes1.drop(crimes1.columns[0], axis=1)
crimes1.head(5)
#df1.describe().T

"""Display the first 5 rows of the dataset with data from years 2005 to 2007"""

import csv
#read the 2nd file with data from 05 to 07
nRowsRead = 1230000  # specify 'None' if you want to read the whole file
crimes2 = pd.read_csv('/content/sample_data/Chicago_Crimes_2005_to_2007.csv', delimiter=',', engine='python', on_bad_lines='skip')
#crimes2 = pd.read_csv('Chicago_Crimes_2005_to_2007.csv', header = None, delimiter=",", quoting=csv.QUOTE_NONE, encoding='utf-8')


nRow, nCol = crimes2.shape
print(f'There are {nRow} rows and {nCol} columns')



#drop unnamed column
#crimes2 = crimes2.drop(crimes2.columns[0], axis=1)
#print(crimes2.head(5))

#there seems to be a problem with line 533719 'expected 23 fields, saw 24'

#print line 533719
#i dropped this line


'''file_path = 'Chicago_Crimes_2005_to_2007.csv'
start_line_number = 533719 - 5  # start a few lines before the problematic one
end_line_number = 533719 + 5    # end a few lines after.

with open(file_path, 'r') as file:
    for current_line_number, line in enumerate(file, 1):
        if start_line_number <= current_line_number <= end_line_number:
            print(f'Line {current_line_number}: {line}')'''

'''Line 533719 appears to be incorrectly formatted. It breaks at the end and starts listing column headers again after
the longitude value. This suggests that the line might have been improperly concatenated with part of another header or line,
or that there was an issue during the generation or editing of the CSV file.'''

#I'd drop this column since we have so many data and manually editing would require lots of time

#CHATGPT: Since you've read the file with on_bad_lines='warn', the problematic line (line 533719 in the CSV) would have been skipped and not included in crimes2

from google.colab import drive
drive.mount('/content/drive')

line_number_to_inspect = 2 #bc python gives me an error when i'm trying to read the file and says the problem is line 284057

with open('Chicago_Crimes_2005_to_2007.csv', 'r') as file:
    for i, line in enumerate(file, start=1):
        if i == line_number_to_inspect or i==line_number_to_inspect+1 or i==line_number_to_inspect+3:
            print(f"Line {i}: {line}")

line_number_to_inspect = 284057 #bc python gives me an error when i'm trying to read the file and says the problem is line 284057

with open('Chicago_Crimes_2005_to_2007.csv', 'r') as file:
    for i, line in enumerate(file, start=1):
        if i == line_number_to_inspect or i==line_number_to_inspect+1 or i==line_number_to_inspect-1 :
            print(f"Line {i}: {line}")

"""Display the first 5 rows of the dataset with data from years 2008 to 2011"""

#read the 3rd file with data from 08 to 11

crimes3 = pd.read_csv('Chicago_Crimes_2008_to_2011.csv', on_bad_lines='warn')


nRow, nCol = crimes3.shape
print(f'There are {nRow} rows and {nCol} columns')

#drop the Unnamed column

crimes3=crimes3.drop(crimes3.columns[0],axis=1)

crimes3.head(5)

"""Display the first 5 rows of the dataset with data from years 2012 to 2017"""

#read the 4th file with data from 12 to 17

crimes4 = pd.read_csv('Chicago_Crimes_2012_to_2017.csv', delimiter=',', engine='python', on_bad_lines='skip')


nRow, nCol = crimes4.shape
print(f'There are {nRow} rows and {nCol} columns')

#drop the Unnamed column
crimes4=crimes4.drop(crimes4.columns[0],axis=1)

crimes4.head(5)

"""Bring the 4 different datasets together #concetanate"""

#bring the 4 different datasets together

crimes = pd.concat([crimes1, crimes2, crimes3, crimes4], ignore_index=False, axis=0)


crimes=crimes.drop('Unnamed: 0', axis=1)

#crimes.info()

"""#1. Data Understanding

Look at the size of your concetanated dataset
"""

num_rows,num_columns=crimes.shape
print(f"Number of rows: {num_rows}, Number of columns: {num_columns}")

"""Show all the columns of the dataset"""

crimes.columns

"""Show the first 5 rows of the dataset"""

crimes.head(5)

"""Get basic information about the dataset"""

crimes.info()

"""Get a summary of the statistics pertaining to the crimes columns"""

#crimes.describe()
crimes.describe([x*0.1 for x in range(10)])
for c in list(crimes):
  print(crimes[c].value_counts())

"""#2. Data Preparation

##Ensure that each value is stored in the correct type

Show the type of each values
"""

crimes.dtypes

"""Convert the 'Date' and 'Updated On' columns from string to datetime data types"""

crimes['Date'] = pd.to_datetime(crimes['Date'], errors='coerce')

# Convert 'Updated On' column to datetime
crimes['Updated On'] = pd.to_datetime(crimes['Updated On'])

print(crimes.dtypes)



"""##Check for duplicates

Given the descriptions of the 'ID' and 'Case Number' features:

'ID' is a unique identifier for each record.
'Case Number' is the Chicago Police Department RD Number, which is unique to each incident.

1. Each record should have a unique identifier ('ID').
2. Each incident should have a unique case number ('Case Number').

Removing rows with duplicate 'ID' or 'Case Number' values ensures data integrity and avoids potential issues that may arise from having multiple records with the same identifier or case number.

See if there are any identical rows
"""

crimes.loc[crimes.duplicated()]
#no identical rows

"""See if there are rows that have the same ID or Case Number"""

crimes.loc[crimes.duplicated(subset=['ID', 'Case Number'])]
# no rows with the same ID or Case Number

"""The following code is not necessary as there are no rows with the same ID or CN"""

print('Dataset Shape before drop_duplicate : ', crimes.shape)
crimes.drop_duplicates(subset=['ID', 'Case Number'], inplace=True)
print('Dataset Shape after drop_duplicate: ', crimes.shape)

crimes.head(5)

"""#Check for missing values

"""

missing_values=crimes.isnull()

missing_counts=missing_values.sum()

print(missing_counts)

"""Calculate percentage of missing values per column"""

# Calculate the percentage of missing values for each column
missing_percentages = (crimes.isnull().sum() / len(crimes)) * 100

# Sort the columns by the percentage of missing values in descending order
missing_percentages = missing_percentages.sort_values(ascending=False)

threshold=15

# Print columns with missing percentages greater than a threshold
high_missing_columns = missing_percentages[missing_percentages > threshold]
print(high_missing_columns)

#for Community Area and Ward, more than 15% of their values are missing

"""Bar chart of missing values"""

import matplotlib.pyplot as plt


# Creating the bar chart
plt.figure(figsize=(10,6))
missing_counts.plot(kind='bar')
plt.title('Missing Values per Feature')
plt.xlabel('Features')
plt.ylabel('Number of Missing Values')
plt.xticks(rotation=45)
plt.show()

"""It might be that many values are missing from a certain year. To check this, I need to:
1. Convert the Date Column to DateTime: Ensure your date column is in a datetime format, so you can extract the year easily. - **We've already done this above**
2. Extract the Year: Create a new column in your DataFrame that contains just the year extracted from the date column.
3. Filter for Missing Values: Identify the rows with missing values in your DataFrame.
4. Group and Count Missing Values per Year: Group the DataFrame by the year and count the number of missing values for each year.
"""

crimes['Year'] = crimes['Date'].dt.year
missing_values_per_year = crimes[crimes.isnull().any(axis=1)].groupby('Year').size()

print(missing_values_per_year)

"""Create a column chart of the missing values per year"""

# Create a column chart
plt.figure(figsize=(10, 6))
missing_values_per_year.plot(kind='bar', color='skyblue')

# Adding titles and labels
plt.title('Missing Values Per Year')
plt.xlabel('Year')
plt.ylabel('Number of Missing Values')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability

# Optionally, add a grid
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.show()

"""Find out the number of rows from the year 2001 and the number of rows from 2001 that have missing values"""

number_of_rows_2001 = crimes[crimes['Year'] == 2001].shape[0]
print(number_of_rows_2001)

rows_with_missing_2001 = crimes[(crimes['Year'] == 2001) & (crimes.isnull().any(axis=1))].shape[0]
print(rows_with_missing_2001)

percentage_missing_rows_2001=(rows_with_missing_2001/number_of_rows_2001)*100

print(percentage_missing_rows_2001)

# more than 99% of the rows in 2001 have missing values
#Therefore I will

""" Determine the number of missing values for each attribute (column) in rows specifically from the year 2001."""

# Filter for rows from the year 2001
rows_2001 = crimes[crimes['Year'] == 2001]

# Count missing values in each column for the year 2001
missing_values_2001 = rows_2001.isnull().sum()

print(missing_values_2001)

"""Show me the rows in 2001 that have values for ward and community area"""

rows_with_values_2001 = crimes[(crimes['Year'] == 2001) &
                               (crimes['Ward'].notnull()) &
                               (crimes['Community Area'].notnull())]

# Display these rows
rows_with_values_2001[['Ward','Community Area']]

#print(crimes.dtypes)

# Step 1: Identify columns with missing values and their data types
columns_with_missing_values = crimes.columns[crimes.isnull().any()]
missing_data_types = crimes[columns_with_missing_values].dtypes

print(missing_data_types)

"""Delete lines for which 50% of the values are missing"""

print(f'Number of rows and columns before: {crimes.shape}')

# Calculate the threshold for keeping rows (50% of total columns)
threshold = 0.50 * len(crimes.columns)

# Drop rows where the number of non-missing values is less than the threshold
crimes = crimes.dropna(axis=0, thresh=threshold)

# Print the DataFrame to see the remaining rows
print(f'Number of rows and columns after:{crimes.shape}')

"""Categorize the features into continuous, discrete and categorical

**Continuous Features:** These are quantitative data that can assume an infinite number of values within a given range. They are often measured rather than counted and can take on decimal values. An example is a merchant’s Gross Merchandise Value (GMV), which can be any value within a range and can include decimals.

**Discrete Features:** These are also quantitative but differ from continuous features in that they assume countable values, usually integers. They represent countable things like the number of sessions, the number of products, etc. An example is a merchant’s number of Sessions.

**Categorical Features:** These are qualitative data that assume a limited, fixed number of possible values, and they represent categories or groups. These values can be numeric or non-numeric (like text) but they don’t have mathematical meaning (you can't perform arithmetic operations on them). An example would be a merchant’s Shopify plan type (like 'Basic', 'Advanced', 'Pro'), where each plan type is a distinct category.

###For all remaining columns that have missing values, use imputation

**Numerical Data (Integer or Float):** For numerical columns, you can use mean, median, or mode imputation.
**Categorical Data (String or Categorical Type):** For categorical columns, mode imputation is usually the most suitable method. You can also consider creating a new category to represent missing values.
**Datetime Data:** For datetime columns, you might consider imputing missing values with the nearest available date or using a forward-fill or backward-fill method.
**Boolean Data:** For boolean columns, you might impute missing values with the most frequent value (mode) or consider using a constant value (e.g., False) to represent missing values.

Continuous (assuming infinite or a large range of values):
Latitude,
Longitude

Discrete (countable, often integer values):
Year,
X Coordinate,
Y Coordinate

Categorical (distinct categories or labels):
ID,
Case Number,
Date,
Block,
IUCR,
Primary Type,
Location Description,
Arrest,
Domestic,
Ward,
Community Area,
FBI Code

Print missing values
"""

missing_values=crimes.isnull()

missing_counts=missing_values.sum()

print(missing_counts)

crimes.head(5)

"""**Location Description**

I have one column called 'Location Description'. It has some missing values and I was wondering if I can infer the missing values from other columns. -> First I will need to find out whether there are related columns.

I will put this back and do it later during the EPA, so I'll leave the column with it's missing values for now

Show me 5 rows that have a nan value for Location Description
"""

crimes[crimes['Location Description'].isna()].head(5)

"""Check if Location Description and Primary Type are related"""

contingency_table = pd.crosstab(crimes['Location Description'], crimes['Primary Type'])
contingency_table

"""Location Description has 153 unique values and Primary Type 35. I decided to limit myself to the 10 most frequent values.

Calculate the 10 most frequent values for both attributes
"""

# Find the top 10 most frequent categories for 'Location Description' and 'Primary Type'
top10_locations = crimes['Location Description'].value_counts().nlargest(10).index
top10_types = crimes['Primary Type'].value_counts().nlargest(10).index

# Filter the DataFrame to include only rows where the values are in the top 10 categories
filtered_crimes = crimes[crimes['Location Description'].isin(top10_locations) & crimes['Primary Type'].isin(top10_types)]

# Generate a new contingency table with the filtered data
new_contingency_table = pd.crosstab(filtered_crimes['Location Description'], filtered_crimes['Primary Type'])
new_contingency_table

"""Create a heatmap"""

# Plot a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(new_contingency_table, annot=True, cmap="YlGnBu", fmt='g')
plt.title('Heatmap of Location Description vs. Primary Type')
plt.xlabel('Primary Type')
plt.ylabel('Location Description')
plt.show()

"""Chi-Square Test of Independence

This test helps determine whether there is a significant association between the two categorical variables. The null hypothesis (H0) is that no association exists.
"""

# Chi-square test on the new contingency table
from scipy.stats import chi2_contingency

chi2, p, dof, expected = chi2_contingency(new_contingency_table)
print(f"Chi-square Statistic: {chi2}, P-value: {p}")

"""A Chi-square statistic of 935,384.08893119 and a P-value of 0.0 indicate a very strong statistical significance, suggesting that there is a highly significant association between "Location Description" and "Primary Type" in your dataset.

After having determined that there is a significant association between Location Description and Primary Type, I can now use group by imputation to impute missing values for Location Description
"""

# This will group your data by 'Primary Type' and then fill in the missing 'Location Description'
# values with the most common 'Location Description' for each 'Primary Type'
crimes['Location Description'] = crimes.groupby('Primary Type')['Location Description'].transform(
    lambda x: x.fillna(x.mode()[0])
)

#test
crimes.isnull().sum()

"""**District**

I could imagine that the district can be derived from other columns, so I will also deal with that later.

Test if there is a correlation between District and Beat

Use a Scatterplot
"""

plt.scatter(crimes['District'], crimes['Beat'])
plt.xlabel('District')
plt.ylabel('Beat')
plt.title('Scatter Plot of District vs Beat')
plt.show()

crimes.head()

"""#Exploratory Data Analysis"""

print(crimes['District'].head())

crimes_by_district = crimes.groupby('District').size()

summary_stats = crimes_by_district.describe()

print(summary_stats)

crimes_by_district = crimes.groupby('District').size()

# Create a bar chart
plt.figure(figsize=(12, 6))
crimes_by_district.plot(kind='bar')
plt.title('Number of Crimes by District')
plt.xlabel('District')
plt.ylabel('Number of Crimes')
plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability
plt.show()

"""Explanation why we categroize the attributes:  This categorization helps us decide what visualizations to choose in our EDA, and what statistical methods we can apply to this data. Some visualizations won’t work on all continuous, discrete, and categorical features. This means we have to treat groups of each type of feature differently. We will see how this works in later sections."""

crimes.dtypes
crimes.head()

"""Creating a bar plot for the number of cases by 'Primary Type'"""

# Group by 'Primary Type' and count the number of cases
cases_by_primary_type = crimes['Primary Type'].value_counts()

# Create the bar plot
plt.figure(figsize=(12, 6))
cases_by_primary_type.plot(kind='bar')

plt.title('Number of Cases by Primary Type')
plt.xlabel('Primary Type')
plt.ylabel('Number of Cases')
#plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability

# Show the plot
plt.show()



"""Get the top 10 most frequent values of the 'Location Description' column"""

cases_location_description=crimes['Location Description'].value_counts()

top_10_locations = cases_location_description.head(10)
print(top_10_locations)

"""Create a bar plot of Location Description"""

plt.figure(figsize=(12, 6))
top_10_locations.plot(kind='bar')
plt.title('Number of Cases by Location Description')
plt.xlabel('Location')
plt.ylabel('Number of Cases')

plt.show()

#EDA

"""
Exploratory Data Anlysis.

In order to observe the relations between crimes and the time of the day, there are some visualisations that could give some valuable ideas to move forward in the project
"""

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
import csv
plt.style.use('ggplot')
pd.set_option('display.max_columns', 200)

#change the path to yours
crimes_sample = pd.read_csv('/Users/pablocarrionmorales/Documents/2nd Semester/Data Engineering/Chicago_Crimes_Project/sample.csv', delimiter=',', engine='python', on_bad_lines='skip')


#comparing which crimes are committed by day and by night

#using pivot table from pandas, which summarize the relatives frequencies of the pairs of variables that we are interested

#Varibles of interest: Time/Type of crime, Time/Month, Time/Area

#Pivot Time/Type
Type_by_Time = crimes_sample.pivot_table(values="IUCR", index= "Location Description", columns= "Night", aggfunc=np.size).fillna(0)

#Pivot Time/Month
Time_by_Month = crimes_sample.pivot_table(values="IUCR", index= "Month", columns="Night", aggfunc=np.size).fillna(0)

#Pivot Time/Area
Time_by_Area = crimes_sample.pivot_table(values="IUCR", index="Community Area", columns="Night",aggfunc=np.size).fillna(0)

#function to normalize the pivot tables and visualize then thwough a heat map
def plot_normalized_heatmap(pivot_table):

    # Normalize the pivot table by column to get relative frequencies by each value
    pivot_table_normalized = pivot_table.div(pivot_table.sum(axis=1), axis=0)

    # Plot the heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_table_normalized, cmap="YlGnBu", fmt=".2f")
    plt.title('Normalized Heatmap')
    plt.xlabel('Night')
    plt.ylabel('Location Description')
    plt.show()

#Heat maps
Heatmap_Typebytime = plot_normalized_heatmap(Type_by_Time)
Heatmap_Timebymonth= plot_normalized_heatmap(Time_by_Month)
Heatmaop_Timebyarea= plot_normalized_heatmap(Time_by_Area)